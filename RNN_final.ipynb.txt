{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb0821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "Dataset Creation\n",
    "def create_dataset():\n",
    "data = \"hello world hello world hello world\"\n",
    "chars = list(set(data))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "# Prepare training data\n",
    "X = [] # Input sequences\n",
    "y = [] # Target sequences\n",
    "for i in range(len(data) - 1):\n",
    "input_char = np.zeros((len(chars)))\n",
    "input_char[char_to_idx[data[i]]] = 1\n",
    "target_char = np.zeros((len(chars)))\n",
    "target_char[char_to_idx[data[i + 1]]] = 1\n",
    "X.append(input_char)\n",
    "y.append(target_char)\n",
    "return np.array(X), np.array(y), char_to_idx, idx_to_char, data\n",
    "Model Parameters\n",
    "class SimpleRNN:\n",
    "def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "# Initialize weights\n",
    "self.Wxh = np.random.randn(hidden_size, input_size) * 0.01 # Input to hidden\n",
    "self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # Hidden to hidden\n",
    "self.Why = np.random.randn(output_size, hidden_size) * 0.01 # Hidden to output\n",
    "# Initialize biases\n",
    "self.bh = np.zeros((hidden_size, 1)) # Hidden bias\n",
    "self.by = np.zeros((output_size, 1)) # Output bias\n",
    "self.learning_rate = learning_rate\n",
    "def sigmoid(self, x):\n",
    "return 1 / (1 + np.exp(-x))\n",
    "def tanh(self, x):\n",
    "return np.tanh(x)\n",
    "def tanh_derivative(self, x):\n",
    "return 1 - np.tanh(x)**2\n",
    "def forward(self, inputs):\n",
    "# Initialize lists to store states\n",
    "self.hidden_states = []\n",
    "self.outputs = []\n",
    "h_prev = np.zeros((self.Whh.shape[0], 1)) # Initial hidden state\n",
    "# Forward pass for each time step\n",
    "for x in inputs:\n",
    "# Convert input to column vector\n",
    "x = x.reshape(-1, 1)\n",
    "# Calculate hidden state\n",
    "h = self.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h_prev) + self.bh)\n",
    "# Calculate output\n",
    "y = self.sigmoid(np.dot(self.Why, h) + self.by)\n",
    "# Store states for backpropagation\n",
    "self.hidden_states.append(h)\n",
    "self.outputs.append(y)\n",
    "h_prev = h\n",
    "return self.outputs\n",
    "def backward(self, inputs, targets, outputs, hidden_states):\n",
    "# Initialize gradients\n",
    "dWxh = np.zeros_like(self.Wxh)\n",
    "dWhh = np.zeros_like(self.Whh)\n",
    "dWhy = np.zeros_like(self.Why)\n",
    "dbh = np.zeros_like(self.bh)\n",
    "dby = np.zeros_like(self.by)\n",
    "dh_next = np.zeros_like(hidden_states[0])\n",
    "# Backpropagate through time\n",
    "for t in reversed(range(len(outputs))):\n",
    "dy = outputs[t] - targets[t].reshape(-1, 1)\n",
    "# Gradients for Why and by\n",
    "dWhy += np.dot(dy, hidden_states[t].T)\n",
    "dby += dy\n",
    "# Gradient for hidden state\n",
    "dh = np.dot(self.Why.T, dy) + dh_next\n",
    "# Gradient through tanh\n",
    "dh_raw = self.tanh_derivative(hidden_states[t]) * dh\n",
    "dbh += dh_raw\n",
    "dWxh += np.dot(dh_raw, inputs[t].reshape(1, -1))\n",
    "dWhh += np.dot(dh_raw, hidden_states[t-1].T) if t > 0 else np.dot(dh_raw, np.zeros_like(hidden_states[0]).T)\n",
    "dh_next = np.dot(self.Whh.T, dh_raw)\n",
    "# Clip gradients to prevent exploding gradients\n",
    "for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "np.clip(dparam, -5, 5, out=dparam)\n",
    "# Update weights and biases\n",
    "self.Wxh -= self.learning_rate * dWxh\n",
    "self.Whh -= self.learning_rate * dWhh\n",
    "self.Why -= self.learning_rate * dWhy\n",
    "self.bh -= self.learning_rate * dbh\n",
    "self.by -= self.learning_rate * dby\n",
    "Training and History\n",
    "def train_and_demonstrate():\n",
    "# Create dataset\n",
    "X, y, char_to_idx, idx_to_char, original_data = create_dataset()\n",
    "# Initialize RNN\n",
    "input_size = len(char_to_idx)\n",
    "hidden_size = 50\n",
    "output_size = len(char_to_idx)\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "# Training loop\n",
    "epochs = 100\n",
    "losses = []\n",
    "print(\"Training the RNN...\")\n",
    "print(\"Original sequence:\", original_data)\n",
    "print(\"\\nTraining Progress:\")\n",
    "for epoch in range(epochs):\n",
    "# Forward pass\n",
    "outputs = rnn.forward(X)\n",
    "# Calculate loss (mean squared error)\n",
    "loss = np.mean([(output - target.reshape(-1, 1))**2 for output, target in zip(outputs, y)])\n",
    "losses.append(loss)\n",
    "# Backward pass\n",
    "rnn.backward(X, y, outputs, rnn.hidden_states)\n",
    "if epoch % 20 == 0 or epoch == epochs - 1:\n",
    "print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "Demonstration and Visualization\n",
    "print(\"\\nDemonstrating predictions for each character in sequence:\")\n",
    "for i in range(len(original_data) - 1):\n",
    "input_char = original_data[i]\n",
    "actual_next_char = original_data[i + 1]\n",
    "# Prepare input\n",
    "input_vector = np.zeros((len(char_to_idx)))\n",
    "input_vector[char_to_idx[input_char]] = 1\n",
    "# Get prediction\n",
    "output = rnn.forward([input_vector])[0]\n",
    "predicted_char = idx_to_char[np.argmax(output)]\n",
    "print(f\"Input: '{input_char}' â†’ Predicted next: '{predicted_char}' (Actual: '{actual_next_char}')\")\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "return rnn, char_to_idx, idx_to_char\n",
    "Run the Demonstration\n",
    "rnn, char_to_idx, idx_to_char = train_and_demonstrate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
